{{- $linux := eq .os "linux" -}}
{{- $podman := and .features.podman (not .features.docker) -}}
---
# yaml-language-server: $schema=https://raw.githubusercontent.com/compose-spec/compose-spec/master/schema/compose-spec.json
services:
  # OTLP collector (scratch image, no healthcheck)
  # Healthcheck: curl -fs http://localhost:13133
  otel-collector:
    image: otel/opentelemetry-collector-contrib:0.145.0
    command: ["--config=/etc/otel-collector-config.yaml"]
    restart: unless-stopped
    ports:
      - "127.0.0.1:4317:4317" # gRPC
      - "127.0.0.1:4318:4318" # HTTP
      # - "127.0.0.1:8888:8888" # Metrics (internal only, Prometheus scrapes via Docker DNS)
    volumes:
      - ./otel-collector-config.yaml:/etc/otel-collector-config.yaml:ro,Z
    deploy:
      resources:
        limits:
          memory: 512M

  # Prometheus (metrics)
  # valet proxy --secure prometheus http://localhost:9090
  prometheus:
    image: prom/prometheus:v3.9.1
    command:
      - --config.file=/etc/prometheus/prometheus.yaml
      - --storage.tsdb.path=/prometheus
      - --storage.tsdb.retention.time=720h # 30 days (default: 15d)
      - --web.enable-lifecycle
      - --web.enable-remote-write-receiver
    restart: unless-stopped
    ports:
      - "127.0.0.1:9090:9090"
    volumes:
      - ./prometheus.yaml:/etc/prometheus/prometheus.yaml:ro,Z
      - ./alerts:/etc/prometheus/alerts:ro,Z
      - prometheus-data:/prometheus
    deploy:
      resources:
        limits:
          memory: 512M
    healthcheck:
      test: ["CMD", "wget", "--spider", "-q", "http://localhost:9090/-/healthy"]
      interval: 30s
      timeout: 10s
      retries: 3
      start_period: 10s

  # Loki (logs, scratch image, no healthcheck)
  # Healthcheck: curl -fs http://localhost:3100/ready
  loki:
    image: grafana/loki:3.6
    command: ["-config.file=/etc/loki/loki.yaml"]
    restart: unless-stopped
    ports:
      - "127.0.0.1:3100:3100"
    volumes:
      - ./loki.yaml:/etc/loki/loki.yaml:ro,Z
      - loki-data:/loki
    deploy:
      resources:
        limits:
          memory: 512M

  # Tempo (traces, distroless image since 2.10, no healthcheck)
  # Healthcheck: wget --spider -q http://localhost:3200/ready
  tempo:
    image: grafana/tempo:2.10.0
    command: ["-config.file=/etc/tempo/tempo.yaml"]
    restart: unless-stopped
    volumes:
      - ./tempo.yaml:/etc/tempo/tempo.yaml:ro,Z
      - tempo-data:/var/tempo
    deploy:
      resources:
        limits:
          memory: 512M

  # Grafana
  # valet proxy --secure grafana http://localhost:3033
  grafana:
    image: grafana/grafana:12.3
    restart: unless-stopped
    ports:
      - "127.0.0.1:3033:3000"
    environment:
      GF_AUTH_ANONYMOUS_ENABLED: "true"
      GF_AUTH_ANONYMOUS_ORG_ROLE: Admin
      GF_AUTH_DISABLE_LOGIN_FORM: "true"
    volumes:
      - ./grafana/provisioning:/etc/grafana/provisioning:ro,Z
      - ./grafana/dashboards:/var/lib/grafana/dashboards:ro,Z
      - grafana-data:/var/lib/grafana
    deploy:
      resources:
        limits:
          memory: 512M
    healthcheck:
      test: ["CMD", "curl", "-fs", "http://localhost:3000/api/health"]
      interval: 30s
      timeout: 10s
      retries: 3
      start_period: 10s

  # Node Exporter: host metrics
  # Healthcheck: curl -fs http://localhost:9100/metrics
  node-exporter:
    image: prom/node-exporter:v1.10.2
    restart: unless-stopped
    {{- if or $linux $podman }}
    privileged: true
    {{- end }}
    volumes:
      - /:/rootfs:ro
      - /proc:/host/proc:ro
      - /sys:/host/sys:ro
    command:
      - "--collector.filesystem.mount-points-exclude=^/(sys|proc|dev|run)($$|/)"
      - "--path.procfs=/host/proc"
      - "--path.rootfs=/rootfs"
      - "--path.sysfs=/host/sys"
    deploy:
      resources:
        limits:
          memory: 128M

  # cAdvisor: container metrics
  cadvisor:
    image: ghcr.io/google/cadvisor:0.56.2
    restart: unless-stopped
    {{- if or $linux $podman }}
    privileged: true
    {{- end }}
    {{- if $podman }}
    command:
      - --podman=unix:///var/run/podman/podman.sock
    {{- end }}
    volumes:
      - /:/rootfs:ro
      - /sys:/sys:ro
      {{- if $podman }}
      # https://github.com/google/cadvisor/issues/3628
      - {{ .chezmoi.homeDir }}/.local/share/containers:/var/lib/containers:ro
      - /dev/disk:/dev/disk:ro
      - /etc/machine-id:/etc/machine-id:ro
      - /run/user/{{ .chezmoi.uid }}:/run/user/{{ .chezmoi.uid }}:ro
      - /sys/fs/cgroup:/sys/fs/cgroup:ro
      - /var/lib/dbus/machine-id:/var/lib/dbus/machine-id:ro
      {{- else }}
      - /var/lib/docker:/var/lib/docker:ro
      # Containerd socket for container name resolution (Docker Desktop Engine 29+)
      # https://github.com/google/cadvisor/pull/3709
      - /run/containerd:/run/containerd:ro
      # Explicit socket: macOS /var/run/docker.sock is a symlink
      - /var/run/docker.sock:/var/run/docker.sock:ro
      {{- end }}
    deploy:
      resources:
        limits:
          memory: 256M
    healthcheck:
      test: ["CMD", "wget", "--spider", "-q", "http://localhost:8080/healthz"]
      interval: 30s
      timeout: 10s
      retries: 3
      start_period: 10s

  # Alertmanager: alert testing and management
  alertmanager:
    image: prom/alertmanager:v0.31.1
    # command: ["--config.file=/etc/alertmanager/alertmanager.yml"]
    restart: unless-stopped
    ports:
      - "127.0.0.1:9093:9093"
    volumes:
      - ./alertmanager.yaml:/etc/alertmanager/alertmanager.yml:ro,Z
      - alertmanager-data:/alertmanager
    deploy:
      resources:
        limits:
          memory: 128M
    healthcheck:
      test: ["CMD", "wget", "--spider", "-q", "http://localhost:9093/-/healthy"]
      interval: 30s
      timeout: 10s
      retries: 3
      start_period: 10s

  # ntfy: push notifications
  ntfy:
    image: binwiederhier/ntfy:v2
    command: serve
    restart: unless-stopped
    # ports:
    #   - "127.0.0.1:8080:80"
    volumes:
      - ./ntfy/server.yaml:/etc/ntfy/server.yml:ro,Z
      - ntfy-cache:/var/cache/ntfy
    deploy:
      resources:
        limits:
          memory: 128M
    healthcheck:
      test: ["CMD", "wget", "-q", "--tries=1", "http://localhost:80/v1/health", "-O", "-"]
      interval: 60s
      timeout: 10s
      retries: 3
      start_period: 5s

  # ntfy-alertmanager: bridges Alertmanager webhooks to readable ntfy messages
  ntfy-alertmanager:
    # https://hub.docker.com/r/xenrox/ntfy-alertmanager/tags
    image: xenrox/ntfy-alertmanager:0.5.0
    restart: unless-stopped
    volumes:
      - ./ntfy-alertmanager.scfg:/etc/ntfy-alertmanager/config:ro,Z
    deploy:
      resources:
        limits:
          memory: 64M
    healthcheck:
      test: ["CMD", "wget", "--spider", "-q", "http://localhost:8080/health"]
      interval: 30s
      timeout: 10s
      retries: 3
      start_period: 5s

  # # Jaeger (traces)
  # jaeger:
  #   image: cr.jaegertracing.io/jaegertracing/jaeger:latest
  #   restart: unless-stopped
  #   ports:
  #     - "127.0.0.1:16686:16686"
  #   deploy:
  #     resources:
  #       limits:
  #         memory: 512M

{{- if .features.ai }}

  # sandboxed.sh: self-hosted AI agent orchestrator (Claude Code, OpenCode, Amp)
  # Runs agents in isolated Linux workspaces with mission control dashboard
  sandboxed-sh:
    # No published image; builds from source a multi-stage
    build: https://github.com/Th0rgal/sandboxed.sh.git#v0.9.1
    restart: unless-stopped
    ports:
      - "127.0.0.1:3030:80"
    volumes:
      - sandboxed-data:/root/.sandboxed-sh
      - claude-auth:/root/.claude
      - opencode-config:/root/.config/opencode
      # - ~/.ssh:/root/.ssh:ro
    environment:
      # Auth
      DASHBOARD_PASSWORD: "${SANDBOXED_DASHBOARD_PASSWORD:-admin}"
      JWT_SECRET: "${SANDBOXED_JWT_SECRET:-change-me-to-a-long-random-string}"
      # DEV_MODE: "false"
      # JWT_TTL_DAYS: "30"

      # Server: Caddyfile hardcodes localhost:3000 as upstream
      HOST: "0.0.0.0"
      # PORT: "3000"
      # MAX_ITERATIONS: "50"
      # STALE_MISSION_HOURS: "24"
      # MAX_PARALLEL_MISSIONS: "1"

      # Backend: overrides auto-detection (claude > opencode > amp > codex)
      DEFAULT_BACKEND: "opencode"

      # Workspace: nspawn needs D-Bus system bus, unavailable in Docker
      SANDBOXED_SH_ALLOW_CONTAINER_FALLBACK: "true"
      WORKING_DIR: /root
      # DESKTOP_ENABLED: "false"

      # Dashboard SSR: should auto-resolve via window.location.origin but may not
      # Configure API URL at http://localhost:3030/settings/backends
      NEXT_PUBLIC_API_URL: "http://localhost:3030"
    # Workspace isolation: nspawn/unshare call clone() which needs full capabilities
    privileged: true
    # nspawn manages its own cgroups, needs access to the host cgroup hierarchy
    cgroup: host
    deploy:
      resources:
        limits:
          memory: 1G
    healthcheck:
      test: ["CMD", "curl", "-fs", "http://localhost:80/api/health"]
      interval: 30s
      timeout: 10s
      retries: 3
      start_period: 30s
{{- end }}

volumes:
  prometheus-data:
  loki-data:
  tempo-data:
  grafana-data:
  alertmanager-data:
  ntfy-cache:
{{- if .features.ai }}
  sandboxed-data:
  claude-auth:
  opencode-config:
{{- end }}
